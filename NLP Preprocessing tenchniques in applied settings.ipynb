{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJXeqYJoXAVjdBRJZdXNTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonginkosi1988/bonginkosi1988/blob/main/NLP%20Preprocessing%20tenchniques%20in%20applied%20settings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNxKHRx-Xu4J",
        "outputId": "879a04c6-7ded-4d90-9546-4cae73cc2e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# install neccessary libraries\n",
        "!pip install nltk spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For sentence and word tokenization\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"spaCy is an amazing NLP library! It's fast and efficient.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TII5UWabYIOU",
        "outputId": "1e56de93-c2d0-4fc7-8cd6-127eb9b9e6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--- NLTK Word Tokenization\n",
        "print(\"NLTK Word Tokens:\")\n",
        "print(word_tokenize(text))\n",
        "\n",
        "#---NLTK Sentence Tokenization\n",
        "print(\"\\nNLTK Sentence Tokens:\")\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LggEFx9faALF",
        "outputId": "e2c1be32-ea75-4a43-9125-be78c437263b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Word Tokens:\n",
            "['spaCy', 'is', 'an', 'amazing', 'NLP', 'library', '!', 'It', \"'s\", 'fast', 'and', 'efficient', '.']\n",
            "\n",
            "NLTK Sentence Tokens:\n",
            "['spaCy is an amazing NLP library!', \"It's fast and efficient.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#---spaCy Tokenization\n",
        "doc = nlp(text)\n",
        "print(\"\\nspaCy Word Tokens:\")\n",
        "print([token.text for token in doc])\n",
        "\n",
        "#---spaCy Sentence Tokenization\n",
        "print(\"\\nspaCy Sentence Tokens:\")\n",
        "print([sent.text for sent in doc.sents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qtHWRKUcEMW",
        "outputId": "5d2b6bc4-ef8f-43f6-a1f0-12d9146ae280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "spaCy Word Tokens:\n",
            "['spaCy', 'is', 'an', 'amazing', 'NLP', 'library', '!', 'It', \"'s\", 'fast', 'and', 'efficient', '.']\n",
            "\n",
            "spaCy Sentence Tokens:\n",
            "['spaCy is an amazing NLP library!', \"It's fast and efficient.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopword Removal & Normalization"
      ],
      "metadata": {
        "id": "rztwgIsYwIhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK Setup\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# spaCy Setup (already downloaded earlier)\n",
        "\n",
        "text = \"Oh wow! This product is actually the BEST I've used so far. No kidding!!\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSQ2zqTAMzS-",
        "outputId": "369f0b6f-628b-4245-920d-45c4b0ce1be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NLTK Approach\n",
        "\n",
        "words = word_tokenize(text.lower())\n",
        "\n",
        "cleaned_nltk = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "print(\"Cleaned Tokens using NLTK:\")\n",
        "\n",
        "print(cleaned_nltk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFaEBwR7NvZu",
        "outputId": "c293963b-3b28-40d2-9576-69720a233f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens using NLTK:\n",
            "['oh', 'wow', 'product', 'actually', 'best', 'used', 'far', 'kidding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- spaCy Approach\n",
        "doc = nlp(text.lower())\n",
        "cleaned_spacy = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
        "print(\"\\nCleaned Tokens using spaCy:\")\n",
        "print(cleaned_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC_pGRYmOT9y",
        "outputId": "6ea8b332-3e76-4bec-c5ae-593b2978653f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned Tokens using spaCy:\n",
            "['oh', 'wow', 'product', 'actually', 'best', 'far', 'kidding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stemming vs. Lemmatization"
      ],
      "metadata": {
        "id": "Q8Ta4iDVPKwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "text = \"Users are loving the new updates. The system runs faster and better.\"\n",
        "tokens = word_tokenize(text.lower())"
      ],
      "metadata": {
        "id": "8kKgeFZdQKdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Stemming\n",
        "\n",
        "stemmed = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
        "\n",
        "print(\"Stemmed Tokens (NLTK):\")\n",
        "\n",
        "print(stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlnupXi5PVbQ",
        "outputId": "60ebb9d7-c2c6-4500-dbdc-aa74f070549d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens (NLTK):\n",
            "['user', 'are', 'love', 'the', 'new', 'updat', 'the', 'system', 'run', 'faster', 'and', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Lemmatization using spaCy\n",
        "doc = nlp(text.lower())\n",
        "lemmatized = [token.lemma_ for token in doc if token.is_alpha]\n",
        "print(\"\\nLemmatized Tokens (spaCy):\")\n",
        "print(lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMifrhI-Qd8J",
        "outputId": "c6ef320e-59b6-4d5a-ef2e-586878312cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Tokens (spaCy):\n",
            "['user', 'be', 'love', 'the', 'new', 'update', 'the', 'system', 'run', 'fast', 'and', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare side by side\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "\n",
        "for token in set(tokens):\n",
        "\n",
        "    if token.isalpha():\n",
        "\n",
        "        print(f\"{token:12s} | Stemmed: {stemmer.stem(token):12s} | Lemma: {nlp(token)[0].lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY2RLBycRZbV",
        "outputId": "33b71f65-1eaa-4695-8595-fbeb91ee4ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison:\n",
            "are          | Stemmed: are          | Lemma: be\n",
            "and          | Stemmed: and          | Lemma: and\n",
            "runs         | Stemmed: run          | Lemma: run\n",
            "the          | Stemmed: the          | Lemma: the\n",
            "better       | Stemmed: better       | Lemma: well\n",
            "updates      | Stemmed: updat        | Lemma: update\n",
            "faster       | Stemmed: faster       | Lemma: fast\n",
            "users        | Stemmed: user         | Lemma: user\n",
            "new          | Stemmed: new          | Lemma: new\n",
            "loving       | Stemmed: love         | Lemma: love\n",
            "system       | Stemmed: system       | Lemma: system\n"
          ]
        }
      ]
    }
  ]
}